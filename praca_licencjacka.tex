\documentclass[a4paper, 12pt,oneside]{book}

% pakiety
\usepackage{polski}
%\usepackage[utf8]{inputenc} % nie można używać as per 
			     % https://tex.stackexchange.com/a/480069/177956
\usepackage{fancyhdr} % nagłówki i stopki
\usepackage{indentfirst} % WAŻNE, MA BYĆ!
\usepackage{graphicx} % to do wstawiania rysunków
\usepackage{amsmath} % to do dodatkowych symboli, przydatne
\usepackage[pdftex,
            left=1in,right=1in,
            top=1in,bottom=1in]{geometry} % marginesy
\usepackage{amssymb} % to też do dodatkowych symboli, też przydatne
\usepackage{pdfpages}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{nameref} % do robienia referencji do chapterów
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgf}
\usetikzlibrary{shapes.geometric, arrows, positioning, angles, quotes}
\graphicspath{ {./img/} }

% tikz shapes for flow diagram
\tikzstyle{terminator} = [rectangle, draw, text centered, rounded corners,
                          minimum height=2em]
\tikzstyle{process} = [rectangle, draw, text centered, minimum height=2em]
\tikzstyle{decision} = [diamond, minimum width=2cm, minimum height=0.5,
			text centered, draw=black]
\tikzstyle{data}=[trapezium, draw, text centered, trapezium left angle=60,
                  trapezium right angle=120, minimum height=2em]
\tikzstyle{connector} = [draw, -latex']
\tikzstyle{arrow} = [thin,->,>=stealth]
% tikz shapes for reinforcement learning figure
\tikzstyle{block} = [rectangle, draw, 
    text width=8em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex]
% tikz shapes for Markov chains
\tikzstyle{state}=[thick,draw=black,circle]
\tikzstyle{reward}=[thick,draw=black,diamond]
\tikzstyle{action}=[thick,draw=black,rectangle]

% listings style taken from https://www.overleaf.com/learn/latex/Code_listing
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% definicje nagłówków i stopek
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
% komenda która sprawi że nie numerowany chapter (chapter*) jest dodany do TOC
\newcommand\chap[1]{%
  \chapter*{#1}%
  \addcontentsline{toc}{chapter}{#1}}
\fancyhf{}
\fancyhead[LE,RO]{\footnotesize\thepage}
\fancyhead[LO]{\footnotesize\rightmark}
\fancyhead[RE]{\footnotesize\leftmark}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}
\addtolength{\headheight}{1.5pt}
\fancypagestyle{plain}{\fancyhead{}\cfoot{\footnotesize\thepage}\renewcommand{\headrulewidth}{0pt}}


% interlinia
\linespread{1.25}


% treść
\begin{document}
% strona tytułowa
\sloppy
\thispagestyle{empty}
\includepdf{strona_tytulowa}
\newpage{}

\thispagestyle{empty}
\newpage{}

% spis treści
\tableofcontents{}
\newpage

% pusta strona - narazie zakomentowana
%\thispagestyle{empty}
%\
%\newpage

\chap{Wstęp}
\textbf{Uczenie maszynowe} (ang. \textit{machine learning}) jest dziedziną
zajmującą się tworzeniem metod, które potrafią się ``uczyć'' to znaczy,
wykorzystywać dane aby poprawiać wydajność w rozwiązywaniu pewnych
problemów\cite{machine_learning_definition}.
Do głównych technik uczenia maszynowego zaliczają się:
\begin{itemize}
	\setlength\itemsep{-0.4em}
\item Uczenie nadzorowane gdzie komputer dostaje przykładowe dane wejściowe i
	pożądane dane wyjściowe, jego celem jest nauczyć się generalnej zasady
	przypisywania danych wejściowych do wyjściowych
\item Uczenie nienadzorowane gdzie dane wejściowe nie są w żaden sposób
	oznaczone a algorytm uczenia musi sam znaleźć ukryte wzory w danych.
\item \textbf{Uczenie przez wzmacnianie}
	(ang. \textit{reinforcement learning}) W odróżnieniu od zarówno uczenia 
	nadzorowanego i nienadzorowanego nie potrzebujemy w tym przypadku
	żadnych	gotowych danych wejściowych i wyjściowych. Zamiast tego,
	algorytm pozyskuje dane na bieżąco ze środowiska, do którego jest
	zastosowany. Następnie wykorzystuje zaobserwowane dane, by osiągnąć
	określony cel.
\end{itemize}

Właśnie dzięki temu, że algorytmy uczenia przez wzmacnianie nie potrzebują do
nauki gotowych
zestawów danych możemy zastosować je do problemów takich jak gra na giełdzie
\cite{trading_reinforcement}, nauka grania w gry, tworzenie autonomicznych
pojazdów, uczenie chodzenia robotów z nogami czy optymalne chłodzenie
serwerowni\cite{reinforcement_applications}.

Celem niniejszej pracy jest zaproponowanie, implementacja oraz analiza
algorytmu uczenia przez wzmacnianie zdolnego do kierowania agentem w popularnej
grze. W toku analizy został wybrany algorytm \textbf{Q-Learning}, który
został zastosowany do wytrenowania agenta realizującego prostą grę ``Flappy
Bird''. Implementacja została zrealizowana z użyciem języka python oraz modułu
pygame.

W rozdziale ``\nameref{chapter:gra}'' przedstawię jaką grę wybrałem, opiszę jej
zasady oraz zaprezentuję szczegóły jej implementacji. Następnie w rozdziale
``\nameref{chapter:uczenie}'' przybliżę zagadnienie uczenia przez
wzmacnianie. Algorytm Q\dywiz learning oraz jego implementację opiszę w
rozdziale ``\nameref{chapter:qlearning}''. Na koniec w rozdziale
``\nameref{chapter:testy}'' przedstawię testy po optymalizacjach algorytmu,
które zastosowałem.
\newpage{}

\chapter{Gra}
\label{chapter:gra}
W celu realizacji założonych celów pracy zdecydowałem się na zaimplementowanie
popularnej gry z 2013 roku ``Flappy Bird'',  autorstwa wietnamskiego developera
Dong Nguyena\cite{flappy_bird_author}. Wybrałem tę grę ze względu na
ograniczoną ilość akcji jaką może podjąć gracz.
\section{Zasady gry} 
\begin{figure}[h] 
	\begin{center}
		\includegraphics[scale=0.40]{flappy_bird.png}
		\caption{Zrzut ekranu z gry.}
		\label{flappy_screenshot}
	\end{center}
\end{figure}

Gracz kontroluje obiekt reprezentowany w grze przez żółtego ptaka.
Zadaniem gracza jest tak nim sterować by omijać obiekty reprezentowane przez 
czerwone rury oraz nie pozwolić na to by siła grawitacji doprowadziła do
kolizji obiektu ptaka z obiektem ziemi (na rysunku \ref{flappy_screenshot}
zielona część na dole). Na obiekt ptaka działa jedynie siła grawitacji a
obiekty rur przesuwane są w lewo w jego stronę ze stałą prędkością. Gracz
może kontrolować jedynie to czy obiekt, którym steruje zwiększy swoją
wysokość o stałą, zakodowaną ilość jednostek czy tego nie zrobi i obiekt
reprezentowany przez ptaka zmniejszy swoją wysokość. Gra kończy się gdy
dojdzie do kolizji obiektu ptaka z, którymkolwiek obiektem rur: dolną
bądź górną albo z obiektem ziemi. Punkty przyznawane są jeżeli graczowi uda
się ominąć nadchodzące przeszkody przelatując górną szczelinę między nimi.
Za każde ominięcie przyznawany jest jeden punkt. Na górze rysunku
\ref{flappy_screenshot} białymi cyframi wypisana jest ilość osiągniętych przez
gracza punktów. W tym przypadku gracz ominął już trzydzieści cztery pary rur.
Podsumowując -- gra sprowadza się do kontrolowania wysokości ptaka tak by ten
przelatywał między nadchodzącymi rurami.

\section{Wykorzystane technologie}
Do zaimplementowania tej gry zdecydowałem się na użycie wysoko poziomowego
interpretowanego języka \textbf{python} w wersji 3.8. Python jest dostępny na
większości współczesnych platform oraz jest to język  open\dywiz source.
Dodatkowo wykorzystałem moduł \textbf{pygame}.

Pygame używa funkcjonalności biblioteki SDL. Simple DirectMedia Layer(SDL)
jest to wieloplatformowa biblioteka zapewniająca nisko poziomowy dostęp do
audio, urządzeń peryferyjnych takich jak mysz, klawiatura, joystick oraz
sprzętu graficznego (za pomocą OpenGL i Direct3D). Jest to technologia open
source, która została zaimplementowana w C. Używana w grach firm takich jak
\textit{Valve Software} czy \textit{SuperTuxCart}\cite{sdl_ref}. Pygame używa
tej biblioteki do odczytywania wejścia z klawiatury/myszy oraz rysowania
wyjścia na ekran.

Pygame pozwala tworzyć w pełni funkcjonalne gry oraz programy multimedialne
w pythonie. Zaletami pygame są między innymi : podstawowe funkcje używają
zoptymalizowanego kodu w C oraz Assemblerze, dostępny podobnie jak python na
większości współczesnych platform. Ponadto jest modularny co pozwala
programiście używać tylko tych komponentów, których naprawdę potrzebuje.
Gier stworzonych z użyciem pygame na samej stronie projektu jest ponad 660
\cite{pygame_about_ref}.
\section{Implementacja}
% YYY ta cała sekcja do poprawy
Głównym elementem implementacji jest nieskończona pętla, w której sprawdzam
zdarzenia, które zaszły, sprawdzam czy mamy wejście od gracza, i aktualizuję
obiekty gry oraz ekran. Na rysunku \ref{game_flow_chart} opisana jako 
\textit{Main game loop}.
\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance = 1.5cm]
\node [terminator] at (0,0) (start) {\textbf{Start}};
\node [process, below of=start] (zainicjalizuj_gre)
{Wczytaj zasoby; zainicjalizuj grę i zegar; zadeklaruj stałe; utwórz grupy};
\node [process, below of=zainicjalizuj_gre] (main_loop) {Main game loop};
\node [decision, below of=main_loop, yshift = -1cm] (900ms) {Minęło $P_f$?};
\node [process, right of=900ms, xshift = 3.5cm] (generate_pipes)
	{Wygeneruj rury};
\node [process, below of=900ms, yshift = -1.7cm] (draw_flappy)
	{Narysuj i zaktualizuj ptaka};
\node [process, below of=draw_flappy] (check_score)
	{Sprawdź czy przyznać punkt};
\node [decision, below of=check_score, yshift = -0.5cm] (check_collision)
	{Kolizja?};
\node [data, below of=check_collision, yshift = -0.9cm] (check_input)
	{Sprawdź input};
\node [decision, below of=check_input, yshift = -0.5cm] (check_esc) {ESC?};
\node [decision, below of=check_esc, yshift = -0.9cm] (check_lpm) {LPM?};
\node [process, right of=check_lpm, xshift = 2.5cm] (flap)
	{Pomachaj skrzydłami};
\node [process, below of=check_lpm, yshift = -0.8cm] (display_update)
	{Zaktualizuj ekran};
\node [terminator, below of=display_update] (end_game) {\textbf{Koniec gry}};
\draw [arrow] (start) -- (zainicjalizuj_gre);
\draw [arrow] (zainicjalizuj_gre) -- (main_loop);
\draw [arrow] (main_loop) -- (900ms);
\draw [arrow] (900ms) -- node[anchor=east] {Nie} (draw_flappy);
\draw [arrow] (draw_flappy) -- (check_score);
\draw [arrow] (check_score) -- (check_collision);
\draw [arrow] (check_collision) -- node[anchor=east] {Nie} (check_input);
\draw [arrow] (check_input) -- (check_esc);
\draw [arrow] (check_esc) -- node[anchor=east] {Nie} (check_lpm);
\draw [arrow] (check_lpm) -- node[anchor=east] {Nie} (display_update);
\draw [arrow] (900ms) -- node[anchor=south] {Tak} (generate_pipes);
\draw [arrow] (check_lpm) -- node[anchor=south] {Tak} (flap);
\draw [arrow] (generate_pipes) |- (draw_flappy);
\draw [arrow] (flap) |- (display_update);
\node [right of = check_collision, xshift = 5cm, inner sep=0pt, outer sep=0pt]
	(phantom_collision) {};
	\draw [arrow] (check_collision) -- node[anchor=south] {Tak}
	(phantom_collision);
\node [right of = check_esc, xshift = 5cm, inner sep=0pt, outer sep=0pt]
	(phantom_esc) {};
\draw [arrow] (phantom_collision) -- (phantom_esc);
\draw [arrow] (check_esc) -- node[anchor=south] {Tak}
	(phantom_esc);
\draw [arrow] (phantom_esc) |- (end_game);
\node [left of = display_update, xshift = -3cm, inner sep=0pt, outer sep=0pt]
	(phantom_display_update) {};
\draw [arrow] (display_update) -- (phantom_display_update);
\draw [arrow] (phantom_display_update) |- (main_loop);
\end{tikzpicture}
\end{center}
\caption{Uproszczony diagram przepływu gry.}
\label{game_flow_chart}
\end{figure}
\newpage{}
Program zaczyna od wczytania wczytania zasobów gry -- obrazków reprezentujących
tło, rurę, ziemię, ptaka. Każdemu z tych zasobów tworzy obiekt klasy
\textit{pygame.image}, wczytywane są za pomocą metody \textit{load}.
Do śledzenia czasu w grze wykorzystywany jest obiekt klasy
\textit{pygame.time.Clock}.

Stałe deklarowane na początku programu to na
przykład: ilość jednostek o ile przesunąć w lewo obiekty rur przy każdym
tyknięciu zegara, szerokość szczeliny między obiektami rur, czy czas po jakim
generować nowe rury. W tym miejscu przypisujemy wartość stałej opisującej co
jaki czas mamy generować nowe obiekty rur. Jest wyliczana ze wzoru:
\begin{equation}
	P_f = 900 * (30 \div F)
\end{equation}
Gdzie $P_f$ to czas po jakim generowane są nowe obiekty rur
(ang. \textit{pipe frequency}), $F$ to ilość klatek na sekundę
(ang. \textit{frames per second}). Dzięki takiemu podejściu gra może częściej
generować obiekty rur przy większej ilości klatek na sekundę, przez co
odległość między nimi zostaje taka sama bez względu na tempo gry.

Przy uruchamianiu programu użytkownik ma opcję wybrać wartość parametru 
klatek na sekundę : 30, 60 bądź 120. Domyślnie jest to 30, stąd we wzorze
znajduje się ta liczba. W ten sposób przy 30 klatkach na sekundę nowe obiekty
rur będą generowane co 900 milisekund, przy 60 co 450 milisekund i co 225
milisekund w przypadku 120 klatek na sekundę. Po kilku próbach stwierdziłem,
że częstsze generowanie obiektów rur sprawiało, że gra była zbyt trudna dla
gracza lub w najgorszym wypadku sprawiało, że gra była nie do przejścia
jak na przykład w scenariuszu przedstawionym na rysunku \ref{frequent_pipes}
gdzie obiekt ptaka nie będzie miał wystarczająco czasu by wznieść się na 
odpowiednią wysokość a potem opaść by ominąć rury. Częstsze generowanie 
obiektów rur nie ma wpływu przy większej ilości klatek na sekundę, gdyż te
przemieszczają się w lewo co tyknięcie zegara o stałą ilość jednostek, a
tykanie zegara uzależnione jest od ilości klatek na sekundę -- większa ilość
klatek na sekundę oznacza częstsze tykanie zegara przez co rury przemieszczają
się z większą prędkością. Rozgrywka staje się zdecydowanie dynamiczniejsza
i przez to również trudniejsza, ale tylko dla ludzi. Q-learningowy agent
dostaje informacje o grze co tyknięcie zegara i jeszcze w tym samym tyknięciu
podejmuje decyzje o ruchu, ale więcej o tym w rozdziale
\nameref{chapter:qlearning}
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.50]{impossible_pipes.png}
		\caption{Często generowane obiekty rur.}
		\label{frequent_pipes}
	\end{center}
\end{figure}

Po określeniu wartości stałych tworzymy trzy instancje klasy
\textit{pygame.sprite.Group}, jedną dla obiektu ptaka, drugą dla obiektów rur
dolnych oraz jedną dla obiektów rur górnych. Będą nam potrzebne później przy
sprawdzaniu czy zaszła kolizja między obiektem ptaka a obiektami rur.
Sam obiekt ptaka jest instancją klasy \textit{Bird} dziedziczącej po
\textit{pygame.sprite.Sprite}. Wspomniana klasa jest przeznaczona dla obiektów,
które mają być widoczne na ekranie, zawiera między innymi
pole na obrazek reprezentujący obiekt oraz metodę
\textit{update}\cite{pygame_sprite_documentation}, którą będę wywoływał co
każdy obrót głównej pętli programowej. W przypadku obiektu ptaka w
przeładowanej metodzie \textit{update} znajduje się logika ``latania'' --
co wywołanie metody update inkrementuję zmienną która mówi o ile ma
być przesunięty obiekt ptaka w dół do maksymalnej wartości ośmiu jednostek.
Jeżeli został wciśnięty lewy przycisk myszy to wartość ta jest zmniejszana o
dziesięć (minusowe wartości tej zmiennej powodują, że obiekt zostanie
przesunięty w górę).

Po tym zaczyna się główna pętla programowa. Co $P_f$ czasu generujemy nowe rury
-- polega to na stworzeniu dwóch instancji klasy \textit{Pipe} dziedziczącej
po \textit{pygame.sprite.Sprite}. Generuję liczbę losową $r$ z zakresu
$(-100, 100)$, która będzie potrzebna w ustaleniu wysokości, na której 
wygeneruję obiekty rur. W konstruktorze nadaję obiektom odpowiedni
obrazek. Po wczytaniu obrazku używam \textit{get\_rect}
będącej metodą klasy \textit{pygame.Surface}, której instancją jest nadany w
konstruktorze obrazek. Zwrócony obiekt przypisuję do pola \textit{rect}.
Metoda ta mapuje obrazek na najmniejszy prostokąt, w którym zmieści się
wczytany obrazek. Mamy dzięki temu dostęp do jego krawędzi, którym możemy
nadawać współrzędne na ekranie, w których ma zostać narysowany.
Wszystkie obiekty rysowane są w przestrzeni kartezjańskiej, gdzie punkt
$(0,0)$ jest lewym górnym rogiem tła.
Górnej lewej krawędzi obiektu dolnej rury nadajemy współrzędne:
\[(W, H \div 2 + r + G \div 2)\]
Natomiast dolnej lewej krawędzi obiektu górnej rury nadajemy współrzędne:
\[(W, H \div 2 + r - G \div 2)\]
Gdzie $W$ to szerokość tła ustalona na początku, $H$ to wysokość tła, a
$G$ to stała oznaczająca odległość między dolną a górną rurą, przypisywana
na początku programu. 
Tak wygenerowane obiekty rur zostaną narysowane zaraz za ekranem z prawej
strony, przesunięte od środka ekranu w pionie o $r$ oraz połowę odległości
ustalonej na początku
działania programu. Tak wygenerowane obiekty dodaję do odpowiednich instancji
\textit{pygame.sprite.Group}. Jako, że klasa \textit{Pipe} dziedziczy po
\textit{pygame.sprite.Sprite} (tak samo jak klasa \textit{Bird}), to powinna
przeładowywać metodę \textit{update}\cite{pygame_sprite_documentation}.
W przypadku klasy reprezentującej rury jest tam tylko logika przesuwania
obiektu w lewo co sprowadza się do zmniejszenia współrzędnej $x$ 
zmiennej \textit{rect} oraz ewentualnym usunięciu go jeżeli współrzędna ta
po zmniejszeniu staje się ujemna (oznacza to, że jest po za lewą krawędzią
ekranu przez co jest nie widoczna i nie potrzebna). Usuwanie polega na
wywołaniu metody \textit{kill}, co powoduje, że obiekt zostaje usunięty
z każdej grupy, której był elementem.

W następnym kroku rysuję na ekranie obiekt ptaka za pomocą metody \textit{draw}
wywoływanej na grupie, do której ten obiekt należy. Metoda ta korzysta z
nadanego w konstruktorze obrazka, oraz zmapowanego obiektu prostokąta
(\textit{sprite.Rect}) do określenia pozycji, na której ma zostać
narysowany\cite{pygame_group_draw_documentation}. Rysowanie obiektów rur odbywa
się w ten sam sposób, z tym, że wspomniana metoda jest wywoływana na
odpowiednio grupie obiektów rur dolnych i górnych. Następnie na grupie, do
której należy obiekt ptaka wywołuję metodę \textit{update}, która obsługuje
logikę ``latania''.

\begin{figure} 
	\begin{center}
		\includegraphics[scale=1.10]{flappy_scoring.png}
		\caption{Granice \textit{rect} obiektów \textit{sprite}.}
		\label{flappy_scoring_fig}
	\end{center}
\end{figure}
Najbliższym z prawej strony obiektem rury jest zawsze obiekt o indeksie zero
z grupy dolnych bądź górnych rur, jest tak dzięki logice metody \textit{update}
klasy \textit{Pipe}. Aby sprawdzić, czy przyznać graczowi punkt sprawdzamy
najpierw, czy obiekt ptaka znalazł się w strefie między dwoma rurami, w tym
celu wystarczy sprawdzić czy współrzędna $x$ pola \textit{rect.left} obiektu
ptaka(na rysunku \ref{flappy_scoring_fig} zwizualizowana jako lewa krawędź
czerwonego prostokąta wokół obiektu ptaka) jest większa tej samej współrzędnej
obiektu dolnej rury. Na rysunku \ref{flappy_scoring_fig} jest to zaznaczone
czarną przerywaną linią z lewej strony. Jeżeli tak jest to ustawiam logiczną
flagę reprezentującą to zajście.
W następnych obrotach głównej pętli programowej sprawdzane jest, czy ta flaga
jest ustawiona i jeżeli tak jest to sprawdzam, czy ptak opuścił strefę między
dwoma rurami. Dzieje się to analogicznie do sprawdzania, czy wszedł z tą
różnicą, że porównuję $x$ pola \textit{rect.right} obiektu dolnej rury (to
znaczy współrzędną prawej krawędzi dolnej rury, na rysunku
\ref{flappy_scoring_fig} oznaczona białą przerywaną linią z prawej strony) z
współrzędną $x$ pola \textit{rect.left} obiektu ptaka. Jeżeli tak się stało to
punkt zostaje przyznany a wcześniej wspomnianą flaga zresetowana. Powody
takiego rozwiązania są dwa:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Jeżeli sprawdzałbym tylko czy ptak ominął tylko lewą, albo tylko
		prawą cześć pola \textit{rect} obiektu dolnej rury to punkty
		byłyby naliczane przy każdym obrocie głównej pętli programowej
		gdy obiekt rury najbliższej z prawej został ominięty a nie
		dotarł jeszcze za lewą granicę ekranu i nie został jeszcze
		usunięty.
	\item Jeżeli przyznawałbym punkt, gdy tylko obiekt ptaka znajdzie się
		w strefie między dwoma obiektami rurami to byłaby szansa, że
		gracz wpadnie na obiekt rury zaraz po przyznaniu punktu --
		byłoby to nieprawidłowe zachowanie
\end{itemize}

Aby sprawdzić, czy zaszła kolizja obiektu ptaka z ziemią wystarczy sprawdzić,
czy współrzędna $y$ dolnej części \textit{rect} (czyli \textit{rect.bottom})
będąca zmienną obiektu ptaka jest większa od $H$ (przypomnę, że w
\textit{pygame} układ współrzędnych kartezjańskich ma swój początek w lewym
górnym rogu ekranu a współrzędne z osi $OY$ ``rosną w dół''), gdzie $H$ to
wysokość tła przypisana na początku. Sprawdzenie kolizji między obiektem ptaka
a obiektami rur dzięki \textit{pygame} sprowadza się do wywołania metody
\textbf{pygame.sprite.groupcollide}, która sprawdza, czy doszło do kolizji
pomiędzy dwiema grupami podanymi jako argumenty i zwraca słownik, gdzie
kluczami są obiekty z grupy podanej jako pierwszy argument a wartościami każdy
obiekt z grupy podanej jako drugi argument, z którym obiekt będący kluczem
wszedł w kolizję. Pod spodem metoda ta sprawdza tak naprawdę czy zmienne
\textit{rect} obiektu z każdej grupy nie nachodzą na
siebie\cite{pygame_groupcollide_documentation}. Jako, że grupa
obiektów \textit{Bird} ma tylko jeden element, to wystarczy, że wspomniana
metoda zwróci cokolwiek by stwierdzić, że zaszła kolizja między obiektem ptaka
a obiektami rur i zakończyć grę.

\textbf{Pygame.event} to moduł, który pozwala na interakcję z kolejką wydarzeń,
które odbywają się w systemie np. wciśnięto lub zwolniono przycisk przycisk
myszy bądź klawiatury, zmieniono rozdzielczość ekranu, wyłączono program.
To ten moduł komunikuje się z biblioteką SDL. Jest zależny od modułu
\textbf{pygame.display}\cite{pygame_event_documentation}, który zarządza
wyświetlaniem okna programu na ekranie.
Okno programu jest inicjalizowane na początku działania programu na podstawie
wymiarów obrazków reprezentujących tło i ziemię w grze. Sprawdzanie wydarzeń
polega na iteracji kolejki, którą \textit{pygame} udostępnia za pomocą metody
\textit{pygame.event.get}. Jeżeli zajdzie się w niej wydarzenie o typie
\textit{pygame.QUIT} albo \textit{pygame.KEYDOWN} (został wciśnięty przycisk
klawiatury) oraz \textit{pygame.event.key} to \textit{pygame.K\_ESCAPE}
(wciśnięty klawisz to escape) to podobnie jak w przypadku wykrycia kolizji
gra jest zakańczana.

Dodatkowo \textit{pygame} umożliwia komunikację ze sprzętem bezpośrednio, nie
przez sprawdzanie kolejki wydarzeń. Mysz obsługiwana jest przez moduł
\textbf{pygame.mouse}. Pozwala na sprawdzenie w każdym momencie, które
przyciski na myszy są wciśnięte za pomocą metody
\textit{pygame.mouse.get\_pressed}, która zwraca tablicę wartości logicznych
reprezentujących przyciski na myszy.
Sprawdzenie, czy gracz wcisnął lewy przycisk myszy jest sprawdzane w metodzie
\textit{update} obiektu ptaka i polega na sprawdzeniu dwóch warunków:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Czy lewy przycisk myszy jest wciśnięty
	\item Czy lewy przycisk myszy był wciśnięty w ostatnim obrocie głównej
		pętli programowej
\end{itemize}
Jeżeli przycisk nie był wciśnięty to obiekt ptaka podlatuje i ustawiana jest
flaga logiczna mówiąca o tym, że lewy przycisk myszy jest wciśnięty. Flaga
ta jest resetowana, gdy przy następnym obrocie głównej pętli programowej
przycisk nie będzie wciśnięty. Takie sprawdzanie jest konieczne, inaczej jedno
wciśnięcie lewego przycisku myszy powodowałoby, że obiekt ptaka podlatywał by
do góry o dużą ilość jednostek, dopóki przycisk nie zostałby zwolniony.

Na końcu aktualizowane jest okno gry dzięki użyciu metody
\textbf{pygame.display.update}, która wysyła zaktualizowany obraz na ekran.
Optymalizacja jaka jest zastosowana w tej metodzie, sprawia, że aktualizowane
są tylko te części obrazu, które faktycznie się zmieniły a nie całość.

Gdy gra zostaje zakończona, na ekranie rysowany przycisk reset oraz ustawiana
flaga logiczna zatrzymująca wykonywanie opisanych w tym rozdziale dyrektyw
dopóki nie zostanie wciśnięty lewy przycisk myszy, wtedy gra jest resetowana,
to znaczy wszystkie zmienne są ustawiane do stanu początkowego co powoduje
rozpoczęcie gry na nowo.


\chapter{Uczenie maszynowe}
\label{chapter:uczenie}
\section{Czym jest sztuczna inteligencja}
% http://artint.info/2e/html/ArtInt2e.Ch1.S1.html
\textbf{Sztuczna inteligencja} (ang. \textit{artificial intelligence}) to
dziedzina nauki, która zajmuje się syntezowaniem oraz analizowaniem
obliczeniowych agentów, którzy potrafią podejmować inteligentne decyzje.

\textbf{Agent} jest czymś, co potrafi podejmować decyzje w pewnym środowisku,
robić coś. Agentami są na przykład: psy, termostaty, ludzie, korporacje,
samoloty czy roboty.

Mówimy, że agent działa/podejmuje decyzje inteligentnie gdy:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Uczy się na podstawie swojego doświadczenia
	\item Jest elastyczny na zmieniające się środowisko i zmieniające się
		cele
	\item Decyzje, które podejmuje są właściwe dla okoliczności, w których
		się znajduje oraz celów, które ma osiągnąć, biorące pod uwagę
		krótko i długo terminowe konsekwencje
\end{itemize}

\textbf{Obliczeniowy agent} to taki agent, którego podjęte decyzje mogą
zostać wyjaśnione za pomocą prymitywnych operacji obliczeniowych
czyli takich, które mogą zostać zaimplementowane na fizycznym urządzeniu
(komputerze). Takie obliczenia mogą przyjmować wiele różnych form.

Wszyscy agenci mają swoje ograniczenia. Żaden agent nie jest wszechmocny czy 
wszechwiedzący. Agenci mogą jedynie obserwować swoje ograniczone środowisko
w bardzo wyspecjalizowanych domenach. Agenci mają skończoną pamięć oraz
określoną ilość czasu na podjęcie decyzji.

Jednym z naukowych celów sztucznej inteligencji jest zrozumienie podstawowych
zasad inteligentnego zachowania, które są zastosowywane zarówno do
naturalnych (zwierzęta) jak i sztucznych
agentów\cite{ai_foundations_scientific_goal}.
Jest on osiągany przez:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Analizę naturalnych i sztucznych agentów
	\item Formułowanie i sprawdzanie hipotez dotyczących czego potrzeba
		by skonstruować inteligentnych agentów
	\item Projektowanie, budowanie  i eksperymentowanie z systemami
		obliczeniowymi, które wykonują zadania powszechnie uważane
		za wymagające inteligencji
\end{itemize}
Inżynieryjnym celem sztucznej inteligencji odpowiadającym wspomnianemu wyżej
celu naukowemu jest projektowanie i syntezowanie
użytecznych inteligentnych artefaktów. Chcemy tworzyć agentów, którzy będą
działać inteligentnie. Tacy agenci są użyteczni w wielu problemach.

\section{Interakcja agenta  ze środowiskiem}
Kwintesencją sztucznej inteligencji jest praktyczne rozumowanie, czyli
rozumowanie w celu osiągnięcia jakiegoś celu. Agenta definiują:
percepcja, rozumowanie oraz podejmowanie działań. Agent podejmuje działania w
\textbf{środowisku}.

Agentem może być na przykład komputer z sensorami i siłownikami, innymi słowy
robot, gdzie jego środowiskiem jest świat rzeczywisty. Może być to autonomiczny
samochód lub jak w moim przypadku może być to program, który
działa w czysto obliczeniowym środowisku to znaczy agent programowy (ang.
\textit{software agent}).
\begin{figure}[!htb] 
\begin{center}
\begin{tikzpicture}[node distance = 6em, auto, thick]
	\node [inner sep=0pt, outer sep=0pt] (Phantom_agent) {};
	\node [left of=Phantom_agent, inner sep=0pt, outer sep=0pt,
		xshift = 1cm] (Phantom_agent_2) {};
	\node [right of=Phantom_agent, inner sep=0pt, outer sep=0pt,
		xshift = -1.1cm] (Phantom_agent_3) {};
	\node [block, below of=Phantom_agent, fill=black, text=white] (Agent)
		{\textbf{Agent}};
	\node [block, below of=Agent] (Environment) {Środowisko};
	\path [line] (Agent.0) --++ (4em,0em) |- node 
	     	[near start]{Akcje} (Environment.0);
	\path [line] (Environment.190) --++ (-6em,0em) |- node
	     	[near start] {Przeszłe doświadczenia} (Agent.170);
	\path [line] (Environment.170) --++ (-4.25em,0em) |- node
	     	[near start, right] {Bodziec} (Agent.190);
	\path [line] (Phantom_agent) --++(0em, 0em) -- node
		[near start, left] {Cele} (Agent.91);
	\path [line] (Phantom_agent_2) --++(0em, 0em) -- node
		[near start, left] {Wcześniejsza wiedza} (Agent.151);
	\path [line] (Phantom_agent_3) --++(0em, 0em) -- node
		[near start, right] {Zdolności} (Agent.31);
\end{tikzpicture}
\end{center}
\caption{Schemat interakcji agenta ze środowiskiem.}
\label{rl_figure}
\end{figure}

Rysunek \ref{rl_figure} pokazuje widok czarnej skrzynki agenta z wejściami
i wyjściami. W dowolnym momencie agent jest zależny od:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item \textbf{Wcześniejszej wiedzy} o sobie samym i środowisku
	\item \textbf{Bodźców} otrzymanych od obecnego
		środowiska, które mogą zawierać obserwacje o
		środowisku, ale również akcje, które środowisko
		narzuca agentowi
	\item \textbf{Przeszłych doświadczeń} z poprzednich
		działań i bodźców lub innych danych, z których
		może się uczyć
	\item \textbf{Celów}, które musi starać się osiągnąć
	\item \textbf{Zdolności}, czyli prymitywnych działań, które
		może wykonać
\end{itemize}
W czarnej skrzynce, agent jest w pewnym stanie wiedzy, może być to wiedza o
środowisku, o tym co próbuje osiągnąć i co zamierza zrobić. Agent aktualizuje
ten stan bazując na bodźcach ze środowiska. Na podstawie stanu wiedzy decyduje
o swoich działaniach\cite{ai_foundations_agents_situated}.

\section{Wiedza agenta} 
% http://artint.info/2e/html/ArtInt2e.Ch3.S2.html
W najprostszym przypadku, agent w celu podjęcia decyzji pozyskuje ze środowiska
(opartego o przestrzeń stanów) stan, ma określony do osiągnięcia cel i nie ma
żadnych niepewności. Agent jest w stanie ustalić jak osiągnąć swój cel
przeszukując przestrzeń stanów środowiska, tak by ze swojego obecnego stanu
przejść do stanu, w którym cel jest osiągnięty. Mając kompletną przestrzeń
stanów, próbuje znaleźć sekwencję działań, których podjęcie sprawi, że osiągnie
cel, zanim podejmie jakiekolwiek działania.

\textbf{Przestrzeń stanów} (ang. \textit{state space}) to jedno z ogólnych
sformułowań inteligentnych działań. \textbf{Stan} zawiera wszystkie informacje
potrzebne do przewidzenia efektów danego działania i ustalenia, czy tenże stan
jest potrzebny do osiągnięcia celu. Przeszukiwanie przestrzeni stanów zakłada,
że:
\begin{itemize}
	\setlength\itemsep{-0.4em}
\item Agent ma całkowitą wiedzę o przestrzeni stanów i zakłada, że istnieje
	stan, w którym będzie obserwował to w jakim stanie się znajduje --
	jest całkowita obserwowalność
\item Agent ma zbiór akcji, które mają znane, deterministyczne efekty na
	środowisko
\item Agent może ustalić czy stan spełnia cel
\end{itemize}
\textbf{Rozwiązanie} to sekwencja działań, które przeniosą agenta z jego
obecnego stanu do stanu, który spełnia cel.

\section{Proces decyzyjny}
W poprzednim podrozdziale założyłem, że rozwiązanie to skończona sekwencja
działań. Co jednak jeżeli problem, który agent musi rozważyć to trwający
proces lub nie wiadomo ile działań agent będzie musiał jeszcze podjąć by
osiągnąć cel. Takie problemy nazywane
są \textbf{problemami nieskończonego horyzontu} (ang. \textit{infinite horizon
problems}), gdzie proces może trwać wiecznie, jak na przykład problem grania
we ``Flappy Bird''. Albo też \textbf{problemami nieokreślonego horyzontu} (ang.
\textit{indefinite horizon problems}), w których agent w końcu zakończy
podejmowanie działań, ale nie wie kiedy to nastąpi.

Dla trwających procesów, może nie mieć sensu rozważanie użyteczności na końcu
procesu ponieważ agent może nigdy nie dojść do końca. Zamiast tego agent może
otrzymać sekwencję \textbf{nagród} (ang. \textit{rewards}).
Negatywne nagrody nazywane są \textbf{karami} (ang. \textit{punishments}).
Problemy niezdefiniowanego horyzontu mogą być wymodelowane używając stanu
stop. \textbf{Stan stop} bądź \textbf{stan absorbujący} to stan, w którym
wszystkie działania nie mają żadnego efektu. To znaczy jeżeli agent znajdzie
się w tym stanie to wszystkie jego działania dostają zerową nagrodę ze
środowiska.

\subsection{Łańcuchy Markova}
% http://artint.info/2e/html/ArtInt2e.Ch8.S2.html
Mówimy, że zmienna $X$ jest \textbf{warunkowo niezależna} od losowej zmiennej
$Y$ zakładając, że mamy zbiór losowych zmiennych $Z_s$ jeżeli
\[P(X | Y, Z_s) = P(X| Z_s)\]
gdzie prawdopodobieństwa są dobrze zdefiniowane (żadne nie jest równe $0$).
Zapis $P(X| Y, Z_s)$ oznacza prawdopodobieństwo zajścia $X$ pod
warunkiem, że zajdzie $Y$ oraz $Z_s$. To oznacza, że dla każdego $x \in
\text{dziedzina}(X)$, dla każdego $y \in \text{dziedzina}(Y)$ oraz dla każdego
$z \in \text{dziedzina}(Z_s)$ jeżeli $P(Y = y \wedge Z_s = z) > 0,$
\[P(X=x|Y=y \wedge Z_s = z) = P(X=x|Z_s = z).\]
To znaczy, mając wartość każdej zmiennej $Z_s$, wiedza o prawdopodobieństwie
$Y$ nie ma wpływu na przekonanie o $X$.

% http://artint.info/2e/html/ArtInt2e.Ch8.S3.html
Pojęcie warunkowej niezależności jest użyte by nadać zwięzłą reprezentację
wielu domenom. Chodzi o to, że jeżeli mamy daną losową zmienną $X$, to może
istnieć zbiór zmiennych $W$, które bezpośrednio wpływają na wartość $X$. W
pewnym sensie $X$ jest warunkowo niezależne od pozostałych zmiennych biorąc pod
uwagę  zmienne $W$. Zbiór lokalnie wpływających na $X$ zmiennych jest nazwany
\textbf{ogrodzeniem Markova} (ang. \textit{Markov blanket}). Ta lokalność
jest wykorzystywana w sieciach przekonań\cite{ai_foundations_belief_networks}.

% http://artint.info/2e/html/ArtInt2e.Ch8.S1.SS3.html#Ch8.Thmtheorem3
\textbf{Sieć przekonań} (ang. \textit{belief network}), również nazywana
\textbf{siecią Bayesa} to acykliczny graf skierowany, gdzie wierzchołki są
losowymi zmiennymi. Ponadto istnieje ścieżka od każdego elementu ze zbioru
$\text{rodzice}(X_i)$ do $X_i$. Z siecią przekonań powiązany jest zbiór
rozkładu prawdopodobieństw warunkowych, które określają warunkowe
prawdopodobieństwo między każdą zmienną a jej rodzicami (co włącza wcześniejsze
prawdopodobieństwa zmiennych bez rodziców). A więc sieć przekonań składa się z:
\begin{itemize}
	\setlength\itemsep{-0.4em}
\item Grafu acyklicznego skierowanego, gdzie każdy wierzchołek jest oznaczony
	losową zmienną
\item Dziedziny dla każdej losowej zmiennej
\item Zbioru rozkładu prawdopodobieństw warunkowych przypisującego
	$P(X|\text{rodzice}(X))$ dla każdej zmiennej $X$
\end{itemize}

% http://artint.info/2e/html/ArtInt2e.Ch8.S5.SS1.html
\textbf{Łańcuch Markova} to sieć przekonań z losowymi zmiennymi ułożonymi w
sekwencję, gdzie każda zmienna bezpośrednio zależy od swojego poprzednika w
sekwencji. Łańcuchy Markova są używane do reprezentowania sekwencji wartości
jak na przykład sekwencja stanów w dynamicznym systemie lub sekwencja słów w
zdaniu. Każdy punkt w sekwencji jest nazywany \textbf{etapem} (ang.
\textit{stage}).
\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}[auto,node distance=15mm,>=latex,font=\small]
    \node[state] (s0) {$s_0$};
    \node[state, right of=s0] (s1) {$s_1$};
    \node[state, right of=s1] (s2) {$s_2$};
    \node[state, right of=s2] (s3) {$s_3$};
    \node[state, right of=s3] (s4) {$s_4$};

    \draw[->] (s0) -- (s1);
    \draw[->] (s1) -- (s2);
    \draw[->] (s2) -- (s3);
    \draw[->] (s3) -- (s4);
\end{tikzpicture}
\caption{Łańcuch Markova jako sieć przekonań.}
\label{markov_chain}
\end{center}
\end{figure}
Rysunek \ref{markov_chain} pokazuje ogólny przypadek łańcucha Markova jako sieć
przekonań. Sieć ma pięć etapów, ale nie musi zatrzymywać się na $s_4$, może
rozrastać się w nieskończoność. Sieci przekonań opierają się na założeniu
niezależności: \[P(S_{i+1} | S_0, \dots , S_i) = P(S_{i+1} | S_i),\]
które jest nazywane \textbf{założeniem Markova}

Często sekwencje są rozłożone w czasie, wtedy $S_t$ reprezentuje stan w
momencie $t$. Intuicyjnie $S_t$ przekazuje całą informację o historii, która
mogłaby wpłynąć na przyszłe stany. Niezależność przypuszczenia w łańcuchu
Markova może być rozumiana jako ``biorąc pod uwagę teraźniejszość -- przyszłość
jest warunkowo niezależna od przeszłości''

Mówimy, że łańcuch Markova jest \textbf{modelem stacjonarnym} (ang.
\textit{stationary model}) oraz \textbf{modelem czasowo homogenicznym } (ang.
\textit{time-homogenous model}) jeżeli wszystkie zmienne mają tę samą dziedzinę
oraz wszystkie prawdopodobieństwa przejść są takie same dla każdego etapu, to
znaczy:
\[\forall i \ge 0, P(S_{i+1}|S_i) = P(S_1|S_0)\]
By określić stacjonarny łańcuch Markova, podano dwa prawdopodobieństwa
warunkowe:
\begin{itemize}
		\setlength\itemsep{-0.4em}
	\item $P(S_0)$ specyfikuje wszystkie początkowe warunki
	\item $P(S_{i+1} | S_i)$ specyfikuje \textbf{dynamikę}, która jest taka
		sama dla każdego $i \ge 0$
\end{itemize}
Stacjonarne łańcuchy Markova interesują nas z kilku względów:
\begin{itemize}
		\setlength\itemsep{-0.4em}
	\item Zapewniają prosty model, który jest łatwy do określenia
	\item Założenie stacjonarności jest często naturalnym modelem, ponieważ
		dynamika środowiska zazwyczaj nie zmienia się w czasie.
	\item Sieć może się rozrastać w nieskończoność. Określenie małej liczby
		parametrów daje nam nieskończoną
		sieć\cite{ai_foundations_markov_chains}.
\end{itemize}

% http://artint.info/2e/html/ArtInt2e.Ch9.S3.html
\subsection{Sieć decyzyjna}
Zazwyczaj agent nie podejmuje decyzji bez wcześniejszych obserwacji środowiska,
nie podejmuje też tylko jednej decyzji. Bardziej typowy scenariusz wygląda
następująco -- agent obserwuje swoje środowisko, decyduje jaką akcję podjąć,
wykonuje tę akcję, obserwuje zmienione po jego akcji środowisko, i tak dalej.
Następujące po sobie (sekwencyjne) akcje agenta mogą zależeć od tego co agent
zaobserwuje, a to co zaobserwuje może zależeć od poprzednich podjętych przez
niego akcji. W takim wypadku, często jedynym powodem dla wykonania jakiejś
akcji jest to by dostarczyć kontekst dla przyszłych akcji. Akcje wykonywane
tylko po to by nabyć informacje nazywane są \textbf{akcjami poszukującymi
informacji}. Formalnie nie muszę odróżniać akcji poszukujących informacji od
innych akcji. Zazwyczaj podjęte akcje będą prowadziły zarówno do pozyskania
nowych informacji jak i wywarciu jakiejś zmiany na środowisku.

\textbf{Sekwencyjny model decyzyjny} specyfikuje:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Jakie akcje są dostępne dla agenta w danym etapie
	\item Jaka informacja jest, albo będzie dostępna dla agenta, kiedy
		będzie musiał podjąć akcję
	\item Efekty akcji na środowisko
	\item To jak pożądane te efekty będą z perspektywy osiągnięcia celu
\end{itemize}

% http://artint.info/2e/html/ArtInt2e.Ch9.S3.SS1.html
\textbf{Sieć decyzyjna} zwana również \textbf{diagramem wpływu} to graficzna
reprezentacja skończonego problemu decyzji
sekwencyjnych\cite{ai_foundations_decision_networks}. Sieć decyzyjna
rozszerza sieć przekonań by zawierać zmienne decyzji (akcji) i użyteczności.

W szczególności, sieć decyzyjna to skierowany graf acykliczny, z następującymi
wierzchołkami:
\begin{itemize}
	\setlength\itemsep{-0.4em}
\item \textbf{Wierzchołek decyzji}, rysowany jako prostokąt reprezentuje
	zmienne decyzji. Agent może wybrać wartość każdej ze zmiennych decyzji.
\item \textbf{Wierzchołek szansy}, rysowany jako okrąg reprezentuje zmienne
	losowe. To są te same wierzchołki co w sieciach przekonań. Każdy
	wierzchołek szansy ma powiązaną ze sobą dziedzinę i warunkowe
	prawdopodobieństwo biorące pod uwagę rodziców wierzchołka. Tak jak w
	sieciach przekonań, rodzice wierzchołka reprezentują warunkową
	zależność: zmienna jest niezależna od innych zmiennych nie będących jej
	potomkami. W sieci decyzyjnej zarówno wierzchołki decyzji i wierzchołki
	szansy mogą być rodzicami wierzchołka szansy.
\item \textbf{Wierzchołek użyteczności}, rysowany jako romb reprezentuje
	użyteczność. Zarówno wierzchołki szansy jak i wierzchołki decyzji mogą
	być rodzicami wierzchołka użyteczności. Mówimy tutaj o użyteczności w
	odniesieniu do wyników akcji agenta -- tego jaki wpływ na jego cel mają
	wyniki jego akcji.
\end{itemize}
\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}[auto,node distance=18mm,>=latex,font=\small]
	\node [state] (weather) {Pogoda};
	\node [draw=none, below of=weather] (help) {};
	\node [action, below of=help] (umbrella) {Parasolka};
	\node [state, left of=help] (forecast) {Prognoza};
	\node [reward, right of=help] (utility) {Użyteczność};
	
	\draw[->, thick] (weather) -- (forecast);
	\draw[->, thick] (forecast) -- (umbrella);
	\draw[->, thick] (umbrella) -- (utility);
	\draw[->, thick] (weather) -- (utility);
\end{tikzpicture}
\caption{Sieć decyzyjna reprezentująca decyzję, czy wziąć parasolkę.}
\label{decision_network_umbrella}
\end{center}
\end{figure}
Krawędzie wchodzące do wierzchołków decyzyjnych reprezentują informację, która
będzie dostępna, jeżeli decyzja zostanie podjęta.
Krawędzie wchodzące do wierzchołków szansy reprezentują zależność
probabilistyczną.
Krawędzie wchodzące do wierzchołków użyteczności reprezentują od czego ta
użyteczność zależy.

Mówimy, że \textbf{agent nie zapomina}, jeżeli decyzje, które podjął są
uporządkowane w czasie, i agent pamięta swoje poprzednie decyzje i każdą
informację, która była dostępna przy poprzedniej decyzji.

Mówimy, że \textbf{sieć decyzyjna nie zapomina}, jeżeli jej wierzchołki decyzji
są całkowicie uporządkowane, to znaczy, jeżeli wierzchołek decyzyjny $D_i$ jest
przed $D_j$ to znaczy, że $D_i$ jest rodzicem $D_j$ i każdy rodzic $D_i$ jest
również rodzicem $D_j$. A to oznacza, że każda informacja dostępna dla $D_i$
jest dostępna dla każdej późniejszej decyzji w sekwencji i akcja wybrana dla
decyzji $D_i$ jest częścią informacji dostępnej dla każdej późniejszej decyzji.

% http://artint.info/2e/html/ArtInt2e.Ch9.S5.html
\subsection{Proces decyzyjny Markova}
Proces decyzyjny Markova, może być rozumiany jako łańcuch Markova powiększony o
działania agenta oraz nagrodę dla agenta. W każdym etapie agent decyduje jakie
działanie podjąć, nagroda i nowy stan zależą od podjętego przez agenta
działania oraz poprzedniego stanu. 

Rozważę jedynie stacjonarne modele łańcuchu Markova, gdzie przejścia między
stanami oraz nagrody nie zależą od czasu. 

Proces decyzyjny Markova (ang. \textit{Markov decision process (MDP)}) składa
się z:
\begin{itemize}
		\setlength\itemsep{-0.4em}
	\item $S$ -- zbiór stanów środowiska
	\item $A$ -- zbiór decyzji (akcji możliwych do podjęcia przez agenta)
	\item $P: S\times S\times A\rightarrow [0,1]$ -- funkcja, która określa
		\textbf{dynamikę}. Zapis $P(s' | s, a)$ oznacza
		prawdopodobieństwo, że agent przejdzie do stanu $s'$ pod
		warunkiem, że był w stanie $s$ i podjął akcję $a$, czyli:
		\[\forall s\in S \ \forall a\in A \sum_{s' \in S}P(s'|s,a)=1\]
	\item $R:S\times S\times A\times S \rightarrow \mathfrak{R}$, gdzie
		$R(s,a,s')$ to \textbf{funkcja nagrody}, dająca oczekiwaną
		natychmiastową nagrodę za wykonanie działania $a$ i przejścia
		do stanu $s'$ ze stanu $s$. Czasem wygodnie jest użyć zapisu
		$R(s,a)$ -- oczekiwana wartość wykonania $a$ w stanie $s$,
		która równa jest $R(s,a)=\sum _{s'}R(s,a,s') * P(s'|s,a)$.
\end{itemize}

\begin{figure}[!htb]
\begin{center}
\begin{tikzpicture}[auto,node distance=18mm,>=latex,font=\small]
    \node[reward] (r0) {$R_0$};
    \node[draw=none, right of=r0] (help_r0) {};
    \node[state, below of=r0, left of=r0] (s0) {$S_0$};
    \node[draw=none, right of=s0] (help_s0) {};
    \node[state, right of=help_s0] (s1) {$S_1$};
    \node[draw=none, right of=s1] (help_s1) {};
    \node[reward, right of=help_r0] (r1) {$R_1$};
    \node[draw=none, right of=r1] (help_r1) {};
    \node[state, right of=help_s1] (s2) {$S_2$};
    \node[draw=none, right of=s2] (help_s2) {};
    \node[reward, right of=help_r1] (r2) {$R_2$};
    \node[state, right of=help_s2] (s3) {$S_3$};
    \node[action, below of=help_s0] (a0) {$A_0$};
    \node[action, below of=help_s1] (a1) {$A_1$};
    \node[action, below of=help_s2] (a2) {$A_1$};

    \draw[->, thick] (s0) -- (s1);
    \draw[->, thick] (s1) -- (s2);
    \draw[->, thick] (s2) -- (s3);
    \draw[->, thick] (s0) -- (a0);
    \draw[->, thick] (s0) -- (r0);
    \draw[->, thick] (a0) -- (r0);
    \draw[->, thick] (a0) -- (s1);
    \draw[->, thick] (s0) -- (r0);
    \draw[->, thick] (s1) -- (r0);
    \draw[->, thick] (s1) -- (r1);
    \draw[->, thick] (s1) -- (a1);
    \draw[->, thick] (a1) -- (r1);
    \draw[->, thick] (a1) -- (s2);
    \draw[->, thick] (s2) -- (r1);
    \draw[->, thick] (s2) -- (r2);
    \draw[->, thick] (s2) -- (a2);
    \draw[->, thick] (a2) -- (r2);
    \draw[->, thick] (a2) -- (s3);
    \draw[->, thick] (s3) -- (r2);
\end{tikzpicture}
\caption{Sieć decyzyjna reprezentująca skończoną część MDP.}
\label{markov_decision_process}
\end{center}
\end{figure}

By zdecydować jaką akcję podjąć agent porównuje różne sekwencje nagród.
Najczęstszym sposobem jest konwersja sekwencji nagród do jednej liczby zwanej
\textbf{nagrodą skumulowaną} (ang. \textit{cumulative reward}). Aby to zrobić
agent łączy obecną nagrodę z innymi nagrodami w przyszłości. Załóżmy, że agent
otrzyma następującą sekwencję nagród: $r_1, r_2, r_3, r_4, \dots$

Trzema najczęstszymi sposobami połączenia sekwencji nagród w nagrodę
skumulowaną $V$ są

\textbf{Nagroda całkowita} (ang. \textit{total reward}) $V =
\sum_{i=1}^{\infty} r_i$. W tym przypadku nagroda skumulowana to suma
wszystkich nagród. Metoda ta działa kiedy możemy zagwarantować, że suma jest
skończona. Jeżeli nie możemy tego założyć to nie możemy określić, która
sekwencja nagród jest preferowana. Na przykład: Sekwencja nagród po 1zł sumuje
się do tej samej $V$ co sekwencja nagród 100zł (obie są nieskończone). Nagroda
jest skończona na przykład w przypadku, gdzie występują stany stopu a agent ma
zawsze nie zerowe prawdopodobieństwo wejścia w taki stan.

\textbf{Średnia nagród} (ang. \textit{average reward}) $V =
\lim_{n\to\infty}\frac{(r_1 + \dots + r_n)}{n}$. W tym przypadku, skumulowana
nagroda agenta to wartość średnia otrzymanych przez niego nagród, uśredniona
dla każdego odcinka czasu. Dopóki nagrody są skończone dopóty $V$ też będzie
wartością skończoną.

\textbf{Dyskontowa nagroda} (ang. \textit{discount reward}) $V = r_1 + \gamma
r_2 + \gamma^2 r_3 + \dots + \gamma^{i-1} r_i + \dots$, gdzie $\gamma$ to stopa
dyskontowa będąca liczbą w zakresie $0 \leq \gamma < 1$. Przy tych założeniach
przyszłe nagrody są warte mniej niż obecna nagroda. Jeżeli $\gamma$ byłaby
równa $1$, to byłby to przypadek całkowitej nagrody. Jeżeli $\gamma = 0$ to
znaczy, że agent ignoruje wszystkie przyszłe nagrody. To, że $0 \leq \gamma <
1$ gwarantuje nam, że jeżeli nagrody są wartościami skończonymi to skumulowana
nagroda również będzie skończona. Nagrodę dyskontową można zapisać jako:
\begin{equation} \label{discount_reward}
\begin{split}
	V &= \sum_{i=1}^{\infty} \gamma^{i - 1} r_i \\
	  &= r_1 + \gamma r_2 + \gamma^2 r_3 + \dots + \gamma^{i-1}r_i +\dots\\
	  &= r_1 + \gamma(r_2 +\gamma(r_3 + \dots)).
\end{split}
\end{equation}
Załóżmy, że $V_k$ to nagroda skumulowana od czasu $k$:
\begin{equation} \label{discount_reward_2}
\begin{split}
	V_k &= r_k + \gamma(r_{k+1} +\gamma(r_{k+2} + \dots))\\
	&= r_k + \gamma V_{k+1}
\end{split}
\end{equation}
W dalszej części pracy będę rozważał nagrodę skumulowaną jako nagrodę
dyskontową.

% http://artint.info/2e/html/ArtInt2e.Ch9.S5.SS1.html
\subsection{Funkcja polityki}
\textbf{Polityka} (ang. \textit{policy}) precyzuje co agent powinien robić
w stanie, w którym obecnie się znajduje\cite{sawka_ml}.
\textbf{Stacjonarna polityka} to funkcja $\pi : S \rightarrow A$ (gdzie $S$ to
zbiór stanów środowiska a $A$ to zbiór dostępnych dla agenta akcji).

Biorąc pod uwagę kryterium przyznania nagrody polityka zawiera spodziewaną
nagrodę skumulowaną dla każdego stanu. Niech $V^{\pi}(s)$ będzie spodziewaną
nagrodą skumulowaną użycia polityki $\pi$ w stanie $s$. To znaczy, jaką nagrodę
skumulowaną agent oczekuje, że otrzyma po zastosowaniu tej polityki w tym
stanie. Mówimy, że $\pi$ jest \textbf{polityką optymalną}, jeżeli nie istnieje
polityka $\pi'$ dla dowolnego stanu $s$ taka, że $V^{\pi'}(s) > V^{\pi}(s)$. To
znaczy, taka polityka, która ma większą spodziewaną nagrodę skumulowaną dla
każdego stanu, niż jakakolwiek polityka optymalna.

Dla problemów nieskończonego horyzontu, stacjonarny proces decyzyjny Markova
zawsze ma optymalną stacjonarną politykę.\cite{ai_foundations_policies}

Jak zatem obliczyć nagrodę skumulowaną optymalnej polityki? Niech $Q^*(s,a)$,
gdzie $s$ to stan, $a$ to akcja będzie oczekiwaną nagrodą skumulowaną wykonania
$a$ w stanie $s$ i potem stosowania optymalnej polityki. Niech $V^*(s)$ będzie
oczekiwaną nagrodą skumulowaną używania optymalnej polityki począwszy od stanu
$s$.
\begin{equation} \label{optimal_policy_1}
\begin{split}
	Q^*(s,a) &= \sum_{s'}P(s'|s,a)(R(s,a,s') + \gamma V^*(s'))\\
	&= R(s,a) + \gamma \sum_{s'}P(s'|s,a) \gamma V^*(s').
\end{split}
\end{equation}
gdzie $R(s,a) = \sum_{s'}P(s'|s,a)R(s,a,s')$ a $s'$ to stan, do którego
przeszedł agent ze stanu $s$ po wykonaniu akcji $a$.
$V^*(s)$ otrzymywane jest przez wykonywanie akcji, która daje najwyższą nagrodę
skumulowaną w każdym stanie:
\begin{equation} \label{optimal_policy_2}
\begin{split}
	V^*(s) &= \max_{a} Q^*(s,a).
\end{split}
\end{equation}
Optymalna polityka $\pi^*$ to jedna z polityk, która daje najlepszą wartość w
każdym stanie:
\begin{equation} \label{optimal_policy_3}
\begin{split}
	\pi^*(s) &= \arg\max_{a} Q^*(s,a).
\end{split}
\end{equation}
Gdzie $\arg\max_{a} Q^*(s,a)$ to funkcja od stanu $s$ a jej wartością jest
akcja $a$, w wyniku wykonania której otrzymamy największą wartość $Q^*(s,a)$.

\section{Uczenie przez wzmacnianie}
Agent uczony przez wzmacnianie działa w środowisku, obserwuje jego stan oraz
otrzymuje nagrody. Na podstawie tych informacji decyduje co ma zrobić. Agenta
charakteryzują następujące cechy:
\begin{itemize}
	\setlength\itemsep{-0.4em}
\item Agent ma dane możliwe stany, w których może się znaleźć oraz zbiór akcji,
	które może wykonać.
\item Za każdym razem, po zaobserwowaniu stanu środowiska wykonuje akcję
\item Celem agenta jest zmaksymalizowanie nagrody dyskontowej, dla jakiejś
	stopy dyskontowej $\gamma$
\end{itemize}
Uczenie przez wzmacnianie może być sformalizowane pod kątem procesu Decyzyjnego
Markova, w którym agent początkowo zna tylko zbiór możliwych stanów oraz zbiór
możliwych akcji. Dynamika $P(s'|a,s)$ oraz funkcja nagrody $R(s,a)$, nie są
agentowi znane. Tak jak w procesie decyzyjnym Markova po każdej akcji agent
obserwuje stan, w którym się znajduje i otrzymuje nagrodę.

% http://artint.info/2e/html/ArtInt2e.Ch12.S3.html#Ch12.E1
\section{Różnice czasowe}
Załóżmy, że mamy sekwencję wartości numerycznych $v_1,v_2,v_3,\dots$ i celem
jest przewidzenie następnej wartości biorąc pod uwagę poprzednie wartości.
Jednym ze sposobów jest obliczenie bieżącego przybliżenia oczekiwanej wartości
$v_i$. Może to być zaimplementowane poprzez utrzymywanie bieżącej średniej:

Niech $A_k$ będzie oszacowaną oczekiwaną wartością bazującą na pierwszych $k$
punktach danych $v_1, \dots, v_k$. Przybliżeniem może być po prostu średnia:
\[A_k = \frac{v_1 + \dots + v_k}{k}\].
Po przemnożeniu przez $k$:
\begin{equation} \label{td_1}
\begin{split}
	k * A_k &= v_1 + \dots + v_{k-1} + v_k \\
	        &= (k-1)A_{k-1} + v_k
\end{split}
\end{equation}
Dzieląc przez $k$ otrzymujemy:
\begin{equation} \label{td_2}
\begin{split}
	A_k &= \left( 1 - \frac{1}{k} \right) * A_{k-1} + \frac{v_k}{k}
\end{split}
\end{equation}
Niech $\alpha_k = \frac{1}{k}$, wtedy:
\begin{equation} \label{td_3}
\begin{split}
	A_k &= (1 - \alpha_k) * A_{k-1} + \alpha_k * v_k \\
	    &= A_{k-1} + \alpha_k * (v_k - A_{k-1})
\end{split}
\end{equation}
Różnicę $v_k - A_{k-1}$ nazywamy \textbf{błędem różnicy czasowej}. Określa jak
bardzo różni się nowa wartość $v_k$, od starej prognozy $A_{k-1}$. Stare
przybliżenie $A_{k-1}$ jest zaktualizowane o $\alpha_k$ razy błąd różnicy
czasowej by otrzymać nowe przybliżenie $A_k$. Interpretacja wzoru na różnicę
czasową jest następująca: jeżeli nowa wartość jest większa niż poprzednia,
zwiększ przewidywaną wartość; jeżeli nowa wartość jest mniejsza niż poprzednia
zmniejsz przewidywaną wartość. Zmiana jest proporcjonalna do różnicy między
nową wartością a poprzednią przewidywaną wartością.

W uczeniu przez wzmacnianie, wartości (skumulowane nagrody) są efektami akcji.
Nowsze wartości są bardziej dokładne od poprzednich ponieważ agent się uczy i
przez to nowsze wartości powinny mieć większą wagę.


\chapter{Algorytm Q-Learning}
\label{chapter:qlearning}
% http://artint.info/2e/html/ArtInt2e.Ch12.S4.html
Autorem algorytmu Q-learning jest Christ Watkins, który zaprezentował go w
swojej pracy doktorskiej zatytułowanej : ``Learning from Delayed
Rewards''\cite{qlearning_inventor}.
W Q-learningu agent próbuje nauczyć się optymalnej polityki z historii
interakcji ze środowiskiem. \textbf{Historia} agenta to sekwencja
stanu-akcji\dywiz nagrody:
\[\langle s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, s_3, a_3, r_4, \dots
\rangle\]
co oznacza, że agent był w stanie $s_0$, wykonał akcję $a_0$ co spowodowało
przyznaniem mu nagrody $r_1$ i przejścia do stanu $s_1$, następnie wykonał
akcję $a_1$, otrzymał nagrodę $r_2$ i przeszedł do stanu $s_2$ i tak dalej.

Historię interakcji traktujemy jako sekwencję doświadczeń, gdzie
\textbf{doświadczenie} to krotka:
\[\langle s, a, r, s' \rangle\]
co oznacza, że agent był w stanie $s$, wykonał akcję $a$ za co otrzymał nagrodę
$r$ i przeszedł do stanu $s'$. Te doświadczenia będą danymi, na podstawie
których agent będzie się uczył co robić. Tak jak w planowaniu decyzji celem
jest by agent zmaksymalizował swoją nagrodę skumulowaną, która zazwyczaj jest
nagrodą dyskontową.

Q-learning wykorzystuje różnice czasowe do obliczenia przybliżonej wartości
$Q^*(s, a)$. W Q-learningu agent utrzymuje tablicę $Q[S,A]$, gdzie $S$ to zbiór
możliwych dla danego środowiska stanów, a $A$ to zbiór akcji, które agent może
podjąć. $Q[s,a]$ reprezentuje obecne przybliżenie $Q^*(s,a)$

Doświadczenie $\langle s,a,r,s' \rangle$ zapewnia jeden punkt danych dla
wartości $Q(s,a)$. Zawiera informację o tym, że agent otrzymał przyszłą wartość
$r + \gamma V(s')$, gdzie $V(s') = \max_{a'}Q(s',a')$ (tutaj $a'$ to akcja
podjęta dla stanu $s'$ czyli taka akcja, która daje najlepszą wartość $Q$).
To jest aktualną rzeczywistą nagrodę plus dyskontowa
oszacowana przyszła nagroda skumulowana. Agent może użyć równania na różnicę
czasową \ref{td_3} by zaktualizować swoje oszacowanie dla $Q(s,a)$:
\begin{equation}
Q[s,a] := Q[s,a] + \alpha * \left(r + \gamma \max_{a'} Q[s',a'] -
Q[s,a]\right)
\label{qvalue_1}
\end{equation}
albo, równoważnie
\begin{equation}
Q[s,a] := (1 - \alpha) * Q[s,a] + \alpha * \left(r + \gamma \max_{a'}
Q[s',a'] \right)
\label{qvalue_2}
\end{equation}
\begin{algorithm}
	\caption{Algorytm Q-Learning\cite{ai_foundations_qlearning}}
\begin{algorithmic}[1]
	\Procedure{Q-Learning}{$S, A, \gamma, \alpha$} \\
	\hspace{\algorithmicindent}\textbf{Zmienne wejściowe} \\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}$S$ jest zbiorem stanów\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}$A$ jest zbiorem akcji\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}$\gamma$ jest stopą
						dyskontową\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}$\alpha$ jest rozmiarem
						jednego kroku\\
	\hspace{\algorithmicindent}\textbf{Zmienne lokalne} \\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}tablica liczb rzeczywistych
					$Q[S,A]$\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}stany $s, s'$\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}akcja $a$\\
	\hspace{\algorithmicindent}zainicjalizuj $Q[S,A]$ zerami\\
	\hspace{\algorithmicindent}zaobserwuj obecny stan $s$\\
	\hspace{\algorithmicindent}\textbf{powtarzaj}\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}wybierz akcję $a$\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}\textit{wykonaj}($a$)\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}otrzymaj nagrodę $r$ i
						zaobserwuj stan $s'$\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent}$Q[s,a] := Q[s,a] + \alpha *
		(r + \gamma * \max_{a'} Q[s',a'] - Q[s,a])$\\
		\hspace{\algorithmicindent}
		\hspace{\algorithmicindent} $s := s'$\\
		\hspace{\algorithmicindent}\textbf{do zakończenia}
	\EndProcedure
\end{algorithmic}
\label{qlearning_algorithm}
\end{algorithm}

Q-Learningowy algorytm uczy się (przybliżenia) optymalnej $Q$-funkcji tak długo
jak agent wystarczająco eksploruje i nie ma ograniczenia co do tego ile razy
może spróbować wykonania akcji w dowolnym stanie.


\section{Implementacja Q-Learningu}
W mojej implementacji Q-Learningu, \textbf{przestrzeń stanów} zdyskretyzowałem
(to znaczy ograniczyłem do zmiennych o skończonej ilości) nad następującymi
parametrami:
\begin{itemize}
	\setlength\itemsep{-0.4em}
	\item Odległość w pionie obiektu ptaka od obiektu najbliższej od prawej
		dolnej rury
	\item Odległość w poziomie obiektu ptaka od obiektu najbliższej od 
		prawej dolnej rury
	\item Prędkość spadania obiektu ptaka.
\end{itemize}
\begin{figure}[!htb]
	\begin{center}
	\includegraphics[scale=0.80]{agent_vision.png}
	\end{center}
	\caption{Wizja agenta.}
	\label{agent_vision}
\end{figure}

Te informacje są zwracane bezpośrednio agentowi od silnika gry, sam 
\textbf{agent nie ma zaimplementowanych} algorytmów wizji komputerowej.

Jak widać na rysunku \ref{agent_vision} agent musi rozpatrywać tylko obiekty
rur, które są po jego prawej stronie, te po lewej zostały ominięte co oznacza,
że agent osiągnął swój cel w odniesieniu do nich. Aby stwierdzić, który obiekt
rury jest najbliższym z prawej nie mogę jak poprzednio (gdzie wystarczyło
ustalić, który obiekt rury jest najbliżej, nie ważne czy z lewej czy z prawej)
wziąć obiektu o indeksie zero z grupy dolnych obiektów rur. Przypomnę, że grupa
obiektów rur działa jak tablica, gdzie obiekty są ponumerowane indeksami od
zera w odniesieniu ich pozycji na ekranie -- obiekt najbardziej z lewej ma
indeks zero. Istnieje scenariusz, w którym agent dopiero co ominął obiekt rury,
ale powinien zacząć już rozpatrywać następne obiekty z prawej strony.
Rozpatrywanie obiektu o indeksie zero z grupy obiektów rur byłoby tutaj błędne.
Wystarczy rozważyć różnicę między współrzędną $x$ obiektu ptaka a obiektu rury
o indeksie zero z grupy obiektów rur dolnych. Jeżeli ta jest większa niż $-30$
jednostek to oznacza, że obiekt został przez agenta ominięty i najbliższą z
prawej rurą będzie ta o indeksie jeden z grupy obiektów rur. W przeciwnym
wypadku najbliższa będzie ta o indeksie zero. Skąd liczba $30$? Jest to lekko
ponad połowa szerokości obiektu rur w pixelach.

Przestrzeń stanów jest inicjalizowana w następujący sposób:
\begin{lstlisting}[language=Python, label={qval_init}, caption={Inicjalizacja
Q-Values}, captionpos=t]
qval = {}
for x in list(range(-80, 510, 10)):
    for y in list(range(-300, 800, 10)):
        for v in range(-10, 9):
            qval[str(x) + "_" + str(y) + "_" + str(v)] = [0, 0]
\end{lstlisting}
Gdzie \textbf{qval} to słownik, którego kluczami są napisy w formacie
$X\_Y\_V$, gdzie $X$ to różnica w poziomie, $Y$ to różnica w pionie, a $V$ to
prędkość spadania. Jest to odpowiednik tablicy $Q[S,A]$ z algorytmu
\ref{qlearning_algorithm}. Wartości $X$ są generowane co dziesięć w zakresie
$[-80,510)$, wartości $Y$ co dziesięć w zakresie $[-300,800)$, a wartość $V$ w
zakresie $[-10,9)$. O tym dlaczego wartości $X$ i $Y$ są generowane co dziesięć
piszę w następnym podrozdziale. Wartość $V$ ma taki zakres, gdyż jest tak
ograniczona przez sam program gry, gdzie wartość $8$ oznacza, że obiekt ptaka
spada najszybciej jak może, a wartość $-10$ to, że podlatuje do góry.

Agent ma możliwość podjąć jedynie \textbf{dwie akcje} -- kliknąć lewy przycisk
myszy (to znaczy podlecieć), albo nie robić nic i pozwolić obiektowi ptaka
zmniejszyć wysokość.

Takim kluczom jest przypisywana tablica z dwoma zerami, gdzie
indeks zerowy jest $Q$ jeżeli dla tego stanu nie została podjęta akcja,
a indeks jeden jest $Q$ dla tego stanu, jeżeli agent zdecydował się
podjąć akcję. Te wyzerowane wartości $Q$ stanowią jedynie symbol zastępczy,
potem będą wyliczane ze wzoru \ref{qvalue_1}.

\textbf{Funkcja nagrody} $R$ przyznaje $1$ punkt za każdym razem, kiedy obiekt
ptaka sterowany przez agenta żyje oraz $-1000$ punktów jeżeli nie żyje.

Jedyną zasadniczą modyfikacją jakiej dokonałem względem algorytmu
\ref{qlearning_algorithm}, jest to, że zamiast aktualizować $Q$ (krok z linii
17 w algorytmie) co każde zaobserwowane doświadczenie, obliczam je od tyłu po
każdej rozegranej przez agenta grze -- tak, że $Q$ jest obliczane od ostatniego
doświadczenia do pierwszego. Pomyślałem, że to pomoże w szybszym propagowaniu
informacji o ``złym stanie'' to znaczy takim, gdzie agent wybrał akcję, która
doprowadziła do śmierci obiektu ptaka. 
\begin{lstlisting}[language=Python, label={qlearning_python},
caption={Zaimplementowany algorytm Q-Learning}, captionpos=t]
def update_scores():
    history = list(reversed(moves))

    t = 1
    for experience in history:
        state = experience[0]
        act = experience[1]
        res_state = experience[2]

        if t == 1 or t == 2:
            cur_reward = reward[1]
        else:
            cur_reward = reward[0]

        qvalues[state][act] = (1 - lr) * (qvalues[state][act]) + lr * \
	    (cur_reward + discount * max(qvalues[res_state]))

        t += 1
    moves = []
\end{lstlisting}
Na listingu \ref{qlearning_python} \textit{moves} to tablica doświadczeń
$(s, a, s')$, gdzie $s$ to poprzedni
stan, $a$ to poprzednia akcja jaką wykonał agent a $s'$ to obecny stan. Tablica
ta jest uzupełniana o nowe doświadczenie, za każdym razem, gdy program gry 
wywołuję metodę agenta proszącą go o wybór akcji. \textit{reward} zwraca
wartość $1$ dla indeksu $0$, oraz wartość $-1000$ dla indeksu $1$. To oznacza,
że dla ostatnich dwóch stanów, jest przyznawana nagroda $-1000$. Dzieje się tak
ponieważ to one, skoro były ostatnie (a metoda jest wywoływana w momencie końca
gry) to doprowadziły do porażki. Pozostałym stanom przyznawana jest nagroda
$1$, ponieważ akcje podjęte dla tych stanów sprawiały, że agent omijał obiekty
rur. \textit{discount} to liczba określająca stopę dyskontową i jest to
stała wynosząca $0.7$ Oczywiście dla każdego doświadczenia aktualizowana jest
wartość $Q$ zgodnie ze wzorem \ref{qvalue_1} i wypisywana odpowiednio do
słownika \textit{qvalues} dla odpowiedniego stanu i akcji. 

\begin{lstlisting}[language=Python, label={act_python},
caption={Metoda wybierająca akcję agenta}, captionpos=t]
def act(xdif, ydif, vel):
    state = map_state(xdif, ydif, vel)
    moves.append((last_state, last_action, state))
    last_state = state

    if qvalues[state][0] >= qvalues[state][1]:
        last_action = 0
        return 0
    else:
        last_action = 1
        return 1
\end{lstlisting}
Program gry przystosowany dla agenta, różni się niewiele od tego
zaprezentowanego na diagramie \ref{game_flow_chart}. Różnice są następujące:
\begin{itemize}
\setlength\itemsep{-0.4em}
\item Nie jest sprawdzane, żadne wejście
\item Co obrót głównej pętli programowej agent jest pytany o swoją akcję, do
	podjęcia której program daje agentowi wcześniej wspomniane wartości,
	które stanowią ``obserwację'' agenta. Sam wybór akcji polega na
	zmapowaniu otrzymanych ze środowiska wartości do tych, które zostały
	zainicjalizowane tak jak jest to pokazane na listingu \ref{qval_init}.
	Czyli w tym przypadku zaokrąglenie wartości $X$ i $Y$ do części
	dziesiątych. A następnie wybieranie akcji, jak pokazuje to
	listing \ref{act_python} -- agent wybiera akcję $1$ (kliknij lewy
	przycisk myszy) albo $0$ (nie rób nic) w zależności od tego dla której
	akcji wartość $Q$ jest większa.
\item W przypadku końca gry wywoływana jest metoda pokazana na listingu
	\ref{qlearning_python},  po czym zaczyna się nowa gra. Gra jest
	resetowana tyle razy ile poda się przy uruchamianiu programu.
\end{itemize}

\chapter{Testy i rezultaty}
\label{chapter:testy}
\section{Optymalizacja pod względem rozmiaru przestrzeni stanów}
\begin{figure}[!htb]
	\begin{center}
		%\input{plots/plotFirstApproach.pgf} YYY
		\includegraphics{plotFirstApproach.png}
	\end{center}
	\caption{Wykres gry od wyniku.}
	\label{plot_first_approach}
\end{figure}

Przestrzeń stanów to zbiór $S$, którego elementy wyglądają następująco:
\[x \in [-80, 509] \cap \mathbb{Z} \ y \in [-300,799] \cap
\mathbb{Z} \ v \in [-10,8] \cap \mathbb{Z}\ \  \{x, y, v\}\]

Co daje nam $(509 + 80 + 1) * (300 + 799 + 1) * (10 + 8 +1) = 12 331 000$
stanów. Wyuczenie agenta -- to znaczy wypełnienie wartości $Q$ dla większości
stanów zajęłoby bardzo dużo czasu. Postanowiłem poświęcić precyzję z jaką agent
wykonuje akcje na rzecz ograniczenia ilości stanów. Dlatego jak w wcześniej
wspominałem stany są inicjalizowane z wartościami $x$ i $y$ generowanymi co
$10$. W ten sposób przestrzeń stanów ma $(50 + 8 + 1) * (30 + 79 + 1) * (10 + 8
+ 1) = 123 310$ stanów, co jak można się spodziewać jest prawie stukrotnym
pomniejszeniem ilości elementów w przestrzeni stanów. Środowisko dalej zwraca
agentowi ``pełne liczby'', jednak ten zanim podejrzy wartość $Q$ mapuje te
wartości zaokrąglając je w dół do części dziesiątych.
Tak zaimplementowany algorytm i grę uruchomiłem po czym otrzymałem wyniki
widoczne na wykresie \ref{plot_first_approach}.

Wykres \ref{plot_first_approach} pokazuje jaką ilość punktów agent otrzymywał w
danej grze -- te dane są zaznaczone na wykresie czarnymi punktami. Agent
rozegrał $5000$ gier. Maksymalny wynik jaki udało mu się osiągnąć to $1003$ w
grze o numerze $4152$. Jest to jednak bardzo odizolowany przypadek. Niebieską
linia to wykres średniej kroczącej. Gdzie średnia ta jest liczona w każdym
punkcie dla ostatnich $100$ gier (z wyjątkiem $100$ pierwszych gier). Po $5000$
gier średnia wynosiła $69,09$.

\section{Optymalizacja algorytmu}
Zauważyłem, że podczas uczenia często miał miejsce scenariusz przedstawiony na
rysunku \ref{agent_top_pipe}.
\begin{figure}[!htb] 
	\begin{center}
		\includegraphics[scale=0.35]{agent_top_pipe_penalty.png}
	\end{center}
	\caption{Agent wybiera złą akcję.}
	\label{agent_top_pipe}
\end{figure}
Gdy wygenerowana szczelina znajdowała się wysoko, a następna za nią
zdecydowanie niżej, agent bardzo często wybierał by podskoczyć zamiast dać czas
obiektowi ptaka na zmniejszenie swojej wysokości co oczywiście sprawiało,
że przegrywał tę grę. Nie mogłem znaleźć przyczyny takiego zachowania dlatego
postanowiłem lekko zmodyfikować algorytm przedstawiony w listingu
\ref{qlearning_python}. Dodałem flagę, która jest włączana, gdy różnica w
pionie między obiektem ptaka a dolną rurą była wystarczająco wysoka. Wtedy,
jeżeli ostatnią akcją agenta było podskoczenie (\textit{last\_action}
$=1$) i flaga była włączona, dla tego stanu przyznawałem nagrodę $-1000$. Po
takim zmodyfikowaniu algorytmu otrzymałem następujące wyniki:
\begin{figure}[!htb]
	\begin{center}
		%\input{plots/plotTopPipeDeathPenalty.pgf} YYY
		\includegraphics{plotTopPipeDeathPenalty.png}
	\end{center}
	\caption{Wykres gry od wyniku po zmianie algorytmu.}
	\label{plot_top_pipe_penalty}
\end{figure}

Agent uczył się tutaj od początku (wszystkie wartości $Q$ zostały wyzerowane
dla każdego stanu przed uruchomieniem programu).
Jak widać na wykresie \ref{plot_top_pipe_penalty} chociaż maksymalny wynik jaki 
udało mu się osiągnąć jest mniejszy niż w poprzedniej grze -- jest to $933$
uzyskanych punktów w grze $4652$ czyli aż o $70$ punktów mniej niż poprzednio
to średni wynik po $5000$ rozegranych gier wynosi już $119,36$, czyli o 
$72.7\%$ więcej. Ponadto w porównaniu do poprzedniej nauki agent po około
$3500$ grach zdecydowanie częściej osiąga wyniki powyżej $200$ punktów, w
porównaniu do poprzedniego podejścia, gdzie nawet po tylu grach zdarzało się,
że agent zdobywał niskie wyniki, właśnie ze względu na zachowanie przedstawione
na rysunku \ref{agent_top_pipe}.

\section{Redukcja optymalizacji w przestrzeni stanów}
Aby jeszcze zwiększyć średni wynik nauczonego agenta postanowiłem zmniejszyć
skalę z jaką zoptymalizowałem rozmiar przestrzeni stanów. Co prawda taki zabieg
wydłuży czas uczenia (więcej stanów oznacza, że agent będzie musiał zagrać
więcej gier, aby obliczyć wartości $Q$ dla każdego z nich), ale zwiększy
precyzje z jaką agent podejmuje decyzje.
\begin{figure}[!htb]
	\begin{center}
		%\input{plots/plotGrid5x5TopPipePenalty.pgf} YYY
		\includegraphics{plotGrid5x5TopPipePenalty.png}
	\end{center}
	\caption{Wykres gry od wyniku po zwiększeniu przestrzeni stanów.}
	\label{plot_gird_5x5}
\end{figure}

Postanowiłem zwiększyć przestrzeń stanów czterokrotnie -- w tym podejściu stany
generuję z dokładnością do wielokrotności liczby $5$ to znaczy wartości $X$ i
$Y$ przedstawione na listingu \ref{qval_init} generowane są co $5$ a nie co
$10$. Analogicznie wartości obserwowane przez agenta ze środowiska są
pomniejszane o resztę z dzielenia przez $5$. W tym przypadku zbiór stanów ma
$493 240$ elementów, co dalej stanowi $\frac{1}{25}$ ilości elementów
zbioru stanów, gdzie żadnej optymalizacji nie zastosowano. W związku z takim
powiększeniem przestrzeni stanów zwiększyłem ilość gier, które rozegrał agent
(ponownie, agent uczył się od nowa, po wyzerowaniu wartości $Q$). Początkowo
chciałem by było to $4$ razy więcej gier niż poprzednio, jednak okazało się,
że agent po około $8000$ gier bardzo często zdobywał wyniki powyżej $200$ co
sprawiało, że rozgrywane gry trwały zdecydowanie dłużej. W tym układzie
rozegranie $15000$ gier zajęło około $36$ godzin przy najszybszym trybie gry
czyli $120$ klatkach na sekundę. Ponownie agentowi nie udało się pobić
największego wyniku z pierwszego podejścia, uzyskał on maksymalnie $907$
punktów, w grze $14424$, co mimo wszystko jest lepszym wynikiem od poprzedniego
podejścia. Średnia w tym podejściu pod koniec była najlepsza i wynosiła
$135.6$, czyli o $13.6\%$ więcej niż poprzednio.

\chap{Podsumowanie}
Przedstawiona w niniejszej pracy problematyka nie wyczerpuje w całości
zagadnienia uczenia przez wzmacnianie, jednak szczegółowo opisuje działanie
algorytmu Q-Learning. Samodzielne zaimplementowanie gry ``Flappy Bird'' pomogło
mi w zrozumieniu i opisaniu jak agent działa w środowisku. Uważam, że wybrane
przeze mnie technologie były trafionym pomysłem, były łatwe w użyciu i
pozwoliły mi na całkowite zrealizowanie mojej pracy.

Największą komplikacją okazał się sam czas uczenia, mimo, że specjalnie dla 
agenta wprowadziłem możliwość uruchomienia szybszej rozgrywki, to przejście
przez agenta $20000$ gier okazało się zabierać zdecydowanie za dużo czasu.
Osiągane przez agenta wyniki mimo moich optymalizacji nie są też zbyt wysokie w
porównaniu do innych rozwiązań, jak na przykład Tonego Xu, któremu udało się
tak zoptymalizować algorytm, by agent w ogóle nie mógł
przegrać\cite{tds_rl_score}. Myślę, że mógłbym bardziej optymalizować swój
algorytm. Potrzebny byłby nakład pracy przyszłej w formie modyfikacji programu
by ten nie przedstawiał graficznej wizualizacji gry (wszak agent potrzebuje
tylko informacji o położeniu najbliższego obiektu rury, nie potrzeba
wizualizacji). Wtedy mógłbym zaimplementować tak główną pętlę programową by ta
nie wykonywała się raz na tyknięcie symulowanego zegara, a tak szybko jak
pozwalałby na to procesor, na którym program został uruchomiony.
	
Niniejsza praca pokazuje jak skomplikowanym i fascynującym zagadnieniem może
być uczenie przez wzmacnianie. Od samego wymyślenia algorytmu do jego
optymalizacji wymaga nie tylko ogromnej wiedzy z zakresu prawdopodobieństwa,
ale i kreatywnego myślenia.

Podsumowując, chociaż osiągane przez agenta wyniki nie są najlepsze (w
porównaniu do innych tego typu rozwiązań, nie do graczy będących ludźmi)
mimo wszystko stosując algorytm uczenia przez wzmacnianie udało mi się
wytrenować agenta do grania w grę, nie mniej jednak zalecany byłby większy
nakład pracy by osiągać naprawdę dobre wyniki.


%\listoftables{} % jeśli są tabele
%\addcontentsline{toc}{chapter}{Spis tabel}
%
%\listoffigures{} % jeśli są obrazki
%\addcontentsline{toc}{chapter}{Spis rysunków}
%
%\lstlistoflistings
%\addcontentsline{toc}{chapter}{Spis listingów}

\addcontentsline{toc}{chapter}{Bibliografia}
% YYY styl bibliografii do poprawy z tym co jest na teams
\bibliographystyle{IEEEtran}
\bibliography{praca_licencjacka}

\end{document}
